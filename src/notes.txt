-----------------------------------------------------------------------------------------------------------------------------
Protocol for Synchronization: 
-----------------------------------------------------------------------------------------------------------------------------


All Nodes: 
	Must check for lock on all local data before processing every time.
	Note: if the data is coming from a different node, the requesting node
	has no idea about the status of a lock, so it's up to the answering node
	to deny the request if the data is locked. For the record, that check does
	happen for reads and writes when the requesting node is looking for a node
	to ask for a piece of data, but that's for programming convenience, and the 
	end result is the same. 
	The important thing is that this check has to take time- the data can't just 
	be magically bypassed if it turns out it's locked (the time was already spent
	decoding it and such).
	
	
Lock Requestor: 
	Asks the owner of "x" for a lock
	Waits for reply..
		Will get: 
			"granted" --> lock is ready 
			"pending" --> still waiting for acks from copyholders 
			"busy"    --> data is already locked
	Tells the owner that the lock is released (Right now this happens immediately after getting "granted")
	(In future, it might be worth it to let any copyholder initiate the lock release, 
	for this implementation it didn't happen. It also may be worth looking into letting
	the lock holder be in charge of telling all the copyholders that the data is free...
	It depends how hotspots tend to form: around requests or data)
	
Lock Owner: 
	When request recieved, check if "x" is already locked
		if yes
			return "busy"
		else 
			return "pending"
			Make the requesting node the lock holder, don't let any other node
			attempt lock the data until this node releases the lock. 
			Go try to lock all of the copyholders
				Once all copyholders "ack", 
					Tell the holder it's "granted"
	When release is received, immediately set the lock holder to null
		Start telling all of the copyholders that the data is released. 
		If another node comes along and wants to lock the data, let it, stop the release operation though

Copyholder who hears set:
	Clear any pending release requests.
	Make sure that the command comes from the lock holder (via the data owner).
	If there are pending updates to that data, respond with that info and determine whether or not to squash those updates. <-- Still not implemented..
	
	
	
Notes: 
		All of this relies on the fact that ALL copies have to acknowledge a lock before it
	gets granted. 
		The response messages from the lock request CAN'T be inferred by an oracle, the consistency
	policies will process them. 
		The whole simulator is built on a static input of on/off data, meaning that energy management policies, 
	such as idling while waiting for lock replies, etc, are going to be lost... so it goes. 
		Assume that when a node receives a lock notification, if it is updating the data getting locked, it will 
	send a notice back to the locking node with the updated data. This prevents the possibility of multiple data 
	versions floating around when data is locked. <--This still needs to be implemented though!!
	
Optimizations: 
		All nodes in listening range could forward the lock and release messages a couple of 
	times.
		A node's response to waiting for a lock needs to be smart, particularly to avoid deadlock and
	optimize energy usage, which will affect probability that a node is on to hear a particular response. 
	
	
Motivation for Single Lock Owner, Multiple Lock Releasors: (though at the moment it's only a Single Lock Releasor policy)

Single Lock Owner-
Assume nodes 0,2,3,4 are copyholders of "x", 1 and 5 are not, any copyholder can take up the lock request.
1 asks 0 for lock on "x"
0 returns "pending", gets ack's from 2,3
5 asks 4 for lock on "x"
4 returns "pending", and goes looking for acks's but 0 is already  halfway through filling 1's request. 
Shouldn't be allowed to happen- means wrong status was sent to 1 and 5

Multiple Lock Releasors-
Assume 0 is "x" owner and 2,3,4 are copyholders, 1 is not a copyholder, but it has a lock on "x". 
1 releases lock on "x", and 2,3,4 hear, but cannot act on it. 
Whole system waits until 0 comes up and the same time as 1 to propogate the message. 

1 releases lock on "x", and 2,3,4 hear and can respond. 
2,3,4 unlock that data
2 continues propogating the message until 0 hears. 
	If 0 gets another request for a lock on "x", it will squash 2's release attempts. 
	
----------------------------------------------------------------------------------------------------------------------------------	
Protocol for Writes
----------------------------------------------------------------------------------------------------------------------------------

To keep track of the versions of different writes, every node keeps a list of active writes (i.e. writes that haven't finished
propogating updates.) Each write keeps track of the key it pertains to, its version number(more about that later) and the copies
that have and haven't acknowledged the update.
To create a write, in addition to initializing all of the acked copies, the invalid times need to be set up. Every time a write happens, 
all of the copies go invalid and additional record of modifying the data is made. The version that a new write uses is based on the most 
recent write version that the node has seen... so things go South really quickly when multiple writers come into the picture!
When a write is added to the list of writes, it runs through the existing list and removes any active writes to the same data and pulls
update messages off of the node's queue. (See the "add_write" funnction).
